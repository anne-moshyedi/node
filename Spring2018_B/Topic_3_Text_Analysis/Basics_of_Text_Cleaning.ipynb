{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Text with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "may need to update nltk after initially installing it: **pip install -U nltk**\n",
    "\n",
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms to know:\n",
    "\n",
    "Tokenizing - word tokenizers, sentence tokenizers\n",
    "\n",
    "Stop Words - meaningless words (or filler words) such as \"like\", \"as\", \"the\", \"it\", etc…\n",
    "\n",
    "Corpora (Corpus) - body of text. ex: medical journals, presidential speeches, etc…\n",
    "\n",
    "Lexicon - words and their meanings. ex: \"well\" can beither an adjective or a noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_text = 'Hello Mr. Smith, how are you doing today? The weather is great and python is awesome. The sky is pinkish-blue. You should not eat cardboard.'\n",
    "\n",
    "# tokenize by sentence\n",
    "print('Tokenized by Sentence:\\n')\n",
    "print(sent_tokenize(example_text))\n",
    "\n",
    "\n",
    "# tokenize by word\n",
    "print('\\nTokenized by word:\\n')\n",
    "print(word_tokenize(example_text))\n",
    "# notice that this counts punctuation as its own word. You can change this parameter if you so choose.\n",
    "\n",
    "# if you want to tokenize the words but exclude punctuation, use this RegexpTokenizer():\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print('\\nTokenized by word without punctuation:\\n')\n",
    "print(tokenizer.tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "example_sentence = 'This is an example showing off stop word filtration.'\n",
    "\n",
    "# NLTK has multiple sets of stop words, but we are going to use the set made specifically for the English language.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('Stop words:\\n')\n",
    "print(stop_words)\n",
    "\n",
    "print('\\nExamples of removing stop words:\\n')\n",
    "      \n",
    "words = word_tokenize(example_sentence)\n",
    "\n",
    "# non-fancy way to filter the stop words out of your text:\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)\n",
    "# notice that only the \"important\" words are left\n",
    "\n",
    "# fancy way to filter the stop words out of your text:\n",
    "filtered_sentence = [word for word in words if not word in stop_words] # basically the for-loop in a single line\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stemming\n",
    "\n",
    "Taking words and \"stemming\" the ends of them. Such as \"-ed\", or \"-ing\" at the end of a word.\n",
    "\n",
    "So, the words \"worked\" and \"working\" would both be converted to \"work\" because they basically have the same root meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['python','pythoner','pythoning','pythoned','pythonly']\n",
    "\n",
    "print('Example words:\\n')\n",
    "for word in example_words:\n",
    "    print(ps.stem(word))\n",
    "\n",
    "print('\\nStemmed sentence:\\n')\n",
    "# Lets see what a stemmed sentence looks like\n",
    "sent_to_stem = 'It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.'\n",
    "\n",
    "words = word_tokenize(sent_to_stem)\n",
    "# we tokenize because PorterStemmer can only stem one word at a time, not an entire sentence.\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming can sometimes act strangely with \"-ly\" endings and, as you can see above, the word \"once\", but overall serves its purpose to take different forms of words and convert them to their root meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from nltk.corpus import state_union # imports a pre-loaded corpus with a bunch of state of the union addresses\n",
    "\n",
    "# get our state of the union address to analyze\n",
    "sample_text = state_union.raw(\"2006-GWBUSH.txt\")\n",
    "\n",
    "# tokenize each word of the text\n",
    "words = nltk.word_tokenize(sample_text)\n",
    "\n",
    "# use nltk.pos_tag() on the words to get their parts of speech in one long list\n",
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "tagged[0:20] # only printed the first 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part of speech tags:\n",
    "\n",
    "CC - coordinating conjunction<br>\n",
    "CD - cardinal digit<br>\n",
    "DT - determiner<br>\n",
    "EX - existential there (like: \"there is\" ... think of it like \"there exists\")<br>\n",
    "FW - foreign word<br>\n",
    "IN - preposition/subordinating conjunction<br>\n",
    "JJ - adjective\t'big'<br>\n",
    "JJR - adjective, comparative\t'bigger'<br>\n",
    "JJS - adjective, superlative\t'biggest'<br>\n",
    "LS - list marker\t1)<br>\n",
    "MD - modal\tcould, will<br>\n",
    "NN - noun, singular 'desk'<br>\n",
    "NNS - noun plural\t'desks'<br>\n",
    "NNP - proper noun, singular\t'Harrison'<br>\n",
    "NNPS - proper noun, plural\t'Americans'<br>\n",
    "PDT - predeterminer\t'all the kids'<br>\n",
    "POS - possessive ending\tparent's<br>\n",
    "PRP - personal pronoun\tI, he, she<br>\n",
    "PRP\\$ - possessive pronoun\tmy, his, hers<br>\n",
    "RB - adverb\tvery, silently<br>\n",
    "RBR - adverb, comparative\tbetter<br>\n",
    "RBS - adverb, superlative\tbest<br>\n",
    "RP - particle\tgive up<br>\n",
    "TO - to\tgo 'to' the store.<br>\n",
    "UH - interjection\terrrrrrrrm<br>\n",
    "VB - verb, base form\ttake<br>\n",
    "VBD - verb, past tense\ttook<br>\n",
    "VBG - verb, gerund/present participle\ttaking<br>\n",
    "VBN - verb, past participle\ttaken<br>\n",
    "VBP - verb, sing. present, non-3d\ttake<br>\n",
    "VBZ - verb, 3rd person sing. present\ttakes<br>\n",
    "WDT - wh-determiner\twhich<br>\n",
    "WP - wh-pronoun\twho, what<br>\n",
    "WP$ - possessive wh-pronoun\twhose<br>\n",
    "WRB - wh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from nltk.corpus import movie_reviews # imports a corpus with a bunch of movie reviews\n",
    "import re\n",
    "\n",
    "\n",
    "# so that we are not finding the frequency of more or less useless things, lets remove stop words and punctuation\n",
    "# all of the words in the corpus:\n",
    "words = movie_reviews.words()\n",
    "# removing stop words:\n",
    "stop_words = set(stopwords.words('english')) # we make this a set because sets are faster than lists\n",
    "filtered_words = [word for word in words if not word in stop_words]\n",
    "\n",
    "\n",
    "# define \"all_words\" as a list that will be all of the words in the corpus (converted to lowercase)\n",
    "all_words = []\n",
    "for word in filtered_words: # movie_reviews.words() is a list of all of the documents tokenized by word\n",
    "    # lets stem and convert the words to lowercase so that we avoid duplicated words and get more meaningful results\n",
    "    if re.match('[a-zA-Z]+', word): # filtering out punctuation\n",
    "        try:\n",
    "            # stem the word\n",
    "            word = ps.stem(word)\n",
    "        except IndexError: \n",
    "            pass\n",
    "        finally:\n",
    "            # convert word to lowercase and append it to all_words\n",
    "            all_words.append(word.lower())\n",
    "\n",
    "\n",
    "# define \"word_frequencies\" as the frequencies of all of the words in \"all_words\"\n",
    "word_frequencies = nltk.FreqDist(all_words)\n",
    "# print the 20 most common words\n",
    "print(word_frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
